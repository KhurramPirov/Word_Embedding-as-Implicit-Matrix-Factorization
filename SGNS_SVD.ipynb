{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec as Matrix Factorization\n",
    "\n",
    "In this assignment you are supposed to apply SVD to training your own [word embedding model](https://en.wikipedia.org/wiki/Word_embedding) which maps English words to vectors of real numbers.\n",
    "\n",
    "Skip-Gram Negative Sampling (SGNS) word embedding model, commonly known as **word2vec** ([Mikolov et al., 2013](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)), is usually optimized by stochastic gradient descent. However, the optimization of SGNS objective can be viewed as implicit matrix factorization objective as was shown in ([Levy and Goldberg, 2015](http://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1. Notation\n",
    "Assume we have a text corpus given as a sequence of words $\\{w_1,w_2,\\dots,w_n\\}$ where $n$ may be larger than $10^{12}$ and $w_i \\in \\mathcal{V}$ belongs to a vocabulary of words $\\mathcal{V}$. A word $c \\in \\mathcal{V}$ is called *a context* of word $w_i$ if they are found together in the text. More formally, given some measure $L$ of closeness between two words (typical choice is $L=2$), a word $c \\in \\mathcal{V}$ is called a context if $c \\in \\{w_{i-L}, \\dots, w_{i-1}, w_{i+1}, \\dots, w_{i+L} \\}$ Let $\\mathbf{w},\\mathbf{c}\\in\\mathbb{R}^d$ be the *word embeddings* of word $w$ and context $c$, respectively. Assume they are specified by the mapping  $\\Phi:\\mathcal{V}\\rightarrow\\mathbb{R}^d$, so $\\mathbf{w}=\\Phi(w)$. The ultimate goal of SGNS word embedding model is to fit a good mapping $\\Phi$.\n",
    "\n",
    "Let $\\mathcal{D}$ be a multiset of all word-contexts pairs observed in the corpus. In the SGNS model, the probability that word-context pair $(w,c)$ is observed in the corpus is modeled as the following distribution:\n",
    "\n",
    "$$\n",
    "P(\\#(w,c)\\neq 0|w,c) = \\sigma(\\mathbf{w}^\\top \\mathbf{c}) = \\frac{1}{1 + \\exp(-\\mathbf{w}^\\top \\mathbf{c})},\n",
    "$$\n",
    "\n",
    "where $\\#(w,c)$ is the number of times the pair $(w,c)$ appears in $\\mathcal{D}$ and $\\mathbf{w}^\\top\\mathbf{c}$ is the scalar product of vectors $\\mathbf{w}$ and $\\mathbf{c}$. Two important quantities which we will also use further are the number of times the word $w$ and the context $c$ appear in $\\mathcal{D}$, which can be computed as\n",
    "\n",
    "$$\n",
    "\\#(w) = \\sum_{c\\in\\mathcal{V}} \\#(w,c), \\quad \\#(c) = \\sum_{w\\in\\mathcal{V}} \\#(w,c).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2. Optimization objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vanilla word embedding models are trained by maximizing log-likelihood of observed word-context pairs, namely\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\sum_{w \\in \\mathcal{V}} \\sum_{c \\in \\mathcal{V}} \\#(w,c) \\log \\sigma(\\mathbf{w}^\\top\\mathbf{c}) \\rightarrow \\max_{\\mathbf{w},\\mathbf{c} \\in \\mathbb{R}^d}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skip-Gram Negative Sampling approach modifies the objective by additionally minimizing the log-likelihood of random word-context pairs, so called *negative samples*. This idea incorporates some useful linguistic information that some number ($k$, usually $k=5$) of word-context pairs *are not* found together in the corpus which usually results in word embeddings of higher quality. The resulting optimization problem is\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\sum_{w \\in \\mathcal{V}} \\sum_{c \\in \\mathcal{V}} \\left( \\#(w,c) \\log \\sigma(\\mathbf{w}^\\top\\mathbf{c}) + k \\cdot \\mathbb{E}_{c'\\sim P_\\mathcal{D}} \\log \\sigma (-\\mathbf{w}^\\top\\mathbf{c}) \\right) \\rightarrow \\max_{\\mathbf{w},\\mathbf{c} \\in \\mathbb{R}^d},\n",
    "$$\n",
    "\n",
    "where $P_\\mathcal{D}(c)=\\frac{\\#(c)}{|\\mathcal{D}|}$ is a probability distribution over word contexts from which negative samples are drawn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Levy and Goldberg, 2015](http://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf) showed that this objective can be equivalently written as\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\sum_{w \\in \\mathcal{V}} \\sum_{c \\in \\mathcal{V}} f(w,c) = \\sum_{w \\in \\mathcal{V}} \\sum_{c \\in \\mathcal{V}} \\left( \\#(w,c) \\log \\sigma(\\mathbf{w}^\\top\\mathbf{c}) + \\frac{k\\cdot\\#(w)\\cdot\\#(c)}{|\\mathcal{D}|} \\log \\sigma (-\\mathbf{w}^\\top\\mathbf{c}) \\right) \\rightarrow \\max_{\\mathbf{w},\\mathbf{c} \\in \\mathbb{R}^d},\n",
    "$$\n",
    "\n",
    "A crucial observation is that this loss function depends only on the scalar product $\\mathbf{w}^\\top\\mathbf{c}$ but not on embedding $\\mathbf{w}$ and $\\mathbf{c}$ separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3. Matrix factorization problem statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $|\\mathcal{V}|=m$, $W \\in \\mathbb{R}^{m\\times d}$ and $C \\in \\mathbb{R}^{m\\times d}$ be matrices, where each row $\\mathbf{w}\\in\\mathbb{R}^d$ of matrix $W$ is the word embedding of the corresponding word $w$ and each row $\\mathbf{c}\\in\\mathbb{R}^d$ of matrix $C$ is the context embedding of the corresponding context $c$. SGNS embeds both words and their contexts into a low-dimensional space $\\mathbb{R}^d$, resulting in the word and context matrices $W$ and $C$. The rows of matrix $W$ are typically used in NLP tasks (such as computing word similarities) while $C$ is ignored. It is nonetheless instructive to consider the product $W^\\top C = M$. Viewed this way, SGNS can be described as factorizing an implicit matrix $M$ of dimensions $m \\times m$ into two smaller matrices.\n",
    "\n",
    "Which matrix is being factorized? A matrix entry $M_{wc}$ corresponds to the dot product $\\mathbf{w}^\\top\\mathbf{c}$ . Thus, SGNS is factorizing a matrix in which each row corresponds to a word $w \\in \\mathcal{V}$ , each column corresponds to a context $c \\in \\mathcal{V}$, and each cell contains a quantity $f(w,c)$ reflecting the strength of association between that particular word-context pair. Such word-context association matrices are very common in the NLP and word-similarity literature. That said, the objective of SGNS does not explicitly state what this association metric is. What can we say about the association function $f(w,c)$? In other words, which matrix is SGNS factorizing? Below you will find the answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solve SGNS optimization problem with respect to the $\\mathbf{w}^\\top\\mathbf{c}$ and show that the matrix being factorized is\n",
    "\n",
    "$$\n",
    "M_{wc} = \\mathbf{w}^\\top\\mathbf{c} = \\log \\left( \\frac{\\#(w,c) \\cdot |\\mathcal{D}|}{k\\cdot\\#(w)\\cdot\\#(c)} \\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** This matrix is called Shifted Pointwise Mutual Information (SPMI) matrix, as its elements can be written as\n",
    "\n",
    "$$\n",
    "\\text{SPMI}(w,c) = M_{wc} = \\mathbf{w}^\\top\\mathbf{c} = \\text{PMI}(w,c) - \\log k\n",
    "$$\n",
    "\n",
    "and $\\text{PMI}(w,c) = \\log \\left( \\frac{\\#(w,c) \\cdot |\\mathcal{D}|}{\\#(w)\\cdot\\#(c)} \\right)$ is the well-known [pointwise mutual information](https://en.wikipedia.org/wiki/Pointwise_mutual_information) of $(w,c)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$x=(w,c)$, $a=\\#(w,c)$,$b=\\#w$,$\\gamma=\\#c$\n",
    "\n",
    "$\\dfrac{df}{dx} = a\\sigma(x)\\exp(-x)+\\frac{kb\\gamma\\sigma(-x)}{D}$\n",
    "\n",
    "$y=\\exp(-x)$\n",
    "For quadratic equations we have two roots one equals to -1, other one to $$-\\frac{kb\\gamma}{Da}$$\n",
    "So $$x=\\log \\left( \\frac{\\#(w,c) \\cdot |\\mathcal{D}|}{k\\cdot\\#(w)\\cdot\\#(c)} \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import sys\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse.linalg import svds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load enwik 8\n",
    "data = np.loadtxt(\"data/enwik8.txt\", dtype=str, delimiter='.')\n",
    "\n",
    "# Create a list of sentences\n",
    "sentences = [data[i].split() for i in data.nonzero()[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Construct the word vocabulary from the obtained sentences which enumerates words which occur more than $r=200$ times in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def create_vocabulary(sentences, r):\n",
    "    voc=Counter()\n",
    "    for i in range(len(sentences)):\n",
    "        dic1={}\n",
    "        wordfreq = [sentences[i].count(p) for p in sentences[i]]\n",
    "        dic1=dict(zip(sentences[i],wordfreq))\n",
    "        for k in dic1:\n",
    "             voc[k]+=dic1[k]\n",
    "            \n",
    "    vocabluary=Counter()\n",
    "    for p in voc:\n",
    "        if voc[p]>=r:\n",
    "            vocabluary[p]+=voc[p]\n",
    "    \n",
    "        \n",
    "            \n",
    "    return {word: (i, freq) for i, (word, freq) in enumerate(vocabluary.items())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab =create_vocabulary(sentences,200)\n",
    "vocabluary=dict(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Scan the text corpus with sliding window of size $5$ and step $1$ (which corresponds to $L$=2) and construct co-occurrence word-context matrix $D$ with elements $D_{wc}=\\#(w,c)$. Ignoring words which occur less than $r=200$ times, but include them into the sliding window. Please, see the graphical illustration of the procedure described."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Sliding window](sliding_window.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus_matrix(sentences, vocab):\n",
    "    vocab_size = len(vocab)\n",
    "    cooccurrences = np.zeros((vocab_size, vocab_size),\n",
    "                                      dtype=np.float64)\n",
    "    L=2\n",
    "    for line in sentences:\n",
    "        words=[word for i, word in enumerate(line)]\n",
    "        positions=[i for i, word in enumerate(line)]\n",
    "        for k in positions:\n",
    "            if words[k] in vocab:\n",
    "                for right_i in range(k+1,min(len(words),k+1+L)):\n",
    "                    if words[right_i] in vocab:\n",
    "                        cooccurrences[vocab[words[k]][0],vocab[words[right_i]][0]]+=1 \n",
    "                        #print(\"ir,jr=\",vocab[words[k]][0],vocab[words[right_i]][0])\n",
    "                                  \n",
    "            \n",
    "                for left_i in range(max(0,k-L),k):\n",
    "                    if words[left_i] in vocab:\n",
    "                        cooccurrences[vocab[words[k]][0],vocab[words[left_i]][0]]+=1\n",
    "                        #print(\"il,jl=\",vocab[words[k]][0],vocab[words[left_i]][0])\n",
    "        words=[]  \n",
    "        positions=[]\n",
    "            \n",
    "    return  cooccurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = create_corpus_matrix(sentences,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "from scipy.sparse import csr_matrix\n",
    "T=csr_matrix(D)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. To find good word embeddings, [Levy and Goldberg, 2015](http://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf) proposed to find rank-$d$ SVD of Shifted Positive Pointwise Mutual Information (SPPMI) matrix\n",
    "\n",
    "$$\n",
    "U \\Sigma V^\\top \\approx \\text{SPPMI},\n",
    "$$\n",
    "\n",
    "where $\\text{SPPMI}(w, c) = \\max\\left(\\text{SPMI}(w, c), 0 \\right)$ and $\\text{SPMI}(w, c)$ is the element of the matrix $\\text{SPPMI}$ at position $(w, c)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.linalg import svds, eigs\n",
    "\n",
    "def compute_embeddings(D, k, d):\n",
    "    n_D=np.sum(D)\n",
    "    n=D.shape[0]\n",
    "    n_w=np.zeros((n,1))\n",
    "    n_w=np.sum(D,axis=1).reshape(n,1)\n",
    "    n_c=np.zeros((1,n))\n",
    "    n_c=np.sum(D,axis=0).reshape(1,n)\n",
    "    emb=np.maximum(np.log(n_D*D)-np.log(n_w.dot(n_c)*k),0)\n",
    "    A=csr_matrix(emb)\n",
    "    u,s,vt=svds(A, d)\n",
    "    ksi=np.diag(s)\n",
    "    embedding_matrix=u.dot(np.sqrt(ksi))\n",
    "    \n",
    "    \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khurram\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: RuntimeWarning: divide by zero encountered in log\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "k = 5 # negative sampling parameter\n",
    "W = compute_embeddings(D, k,200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Write class **WordVectors** using provided template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class WordVectors:\n",
    "    \n",
    "    def __init__(self, vocabulary, embedding_matrix):\n",
    "        self.vocab = vocabulary\n",
    "        self.W = embedding_matrix\n",
    "        self.inv_vocab = {v: k for k, v in self.vocab.items()}\n",
    "        \n",
    "    def word_vector(self, word):\n",
    "        \"\"\" \n",
    "        Takes word and returns its word vector.\n",
    "        \"\"\"\n",
    "        i=self.vocab[word][0]\n",
    "        word_vector=self.W[i]\n",
    "        return word_vector\n",
    "    \n",
    "    def nearest_words(self, word, top_n=10):\n",
    "        \"\"\" \n",
    "        Takes word from the vocabulary and returns its top_n\n",
    "        nearest neighbors in terms of cosine similarity.\n",
    "        \"\"\"\n",
    "        top=dict()\n",
    "        w1=self.word_vector(word)\n",
    "        for words in self.vocab.keys():\n",
    "            w2=self.word_vector(words)\n",
    "            top[words]=(np.dot(w1.T,w2))/(np.linalg.norm(w1,2)*np.linalg.norm(w2,2))\n",
    "        \n",
    "        sorted_by_value = sorted(top.items(),reverse=True, key=lambda kv: kv[1])\n",
    "                            \n",
    "                                         \n",
    "        return sorted_by_value[1:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WordVectors(vocab, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.word_vector(\"numerical\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.word_vector(\"rap\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5787"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07692437137574666"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.word_vector(\"rap\").T.dot(model.word_vector(\"numerical\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('computation', 0.5465058022397276),\n",
       " ('mathematical', 0.5315278316790778),\n",
       " ('calculations', 0.49945706919609484),\n",
       " ('polynomial', 0.48538065620585513),\n",
       " ('calculation', 0.47300081538676186),\n",
       " ('practical', 0.46014260627224873),\n",
       " ('statistical', 0.4555170951220393),\n",
       " ('symbolic', 0.4549752576135998),\n",
       " ('geometric', 0.44109344247751925),\n",
       " ('simplest', 0.4379533370040826)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.nearest_words(\"numerical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('differential', 0.7594258959609007),\n",
       " ('equations', 0.7239119589985703),\n",
       " ('equation', 0.6819562007574621),\n",
       " ('continuous', 0.6742284479542099),\n",
       " ('multiplication', 0.6736136605499364),\n",
       " ('integral', 0.6720418396042824),\n",
       " ('algebraic', 0.6671381718198981),\n",
       " ('vector', 0.6540550898958007),\n",
       " ('algebra', 0.6302501789190702),\n",
       " ('inverse', 0.6221151413953532)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.nearest_words(\"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('calculus', 0.4458928642823162),\n",
       " ('geometry', 0.4384216472756925),\n",
       " ('equations', 0.3445289877801375),\n",
       " ('equation', 0.31410073600328614),\n",
       " ('topology', 0.3000525510451743),\n",
       " ('arithmetic', 0.2904635768230834),\n",
       " ('mathematics', 0.2861685612477852),\n",
       " ('differential', 0.2856547573415083),\n",
       " ('algebraic', 0.2806914372666656),\n",
       " ('integral', 0.2596334028163852)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.nearest_words(\"algebra\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('communism', 0.7916811289121348),\n",
       " ('anarcho', 0.7866274463952377),\n",
       " ('capitalism', 0.7835661470696771),\n",
       " ('socialism', 0.7521567332112503),\n",
       " ('liberalism', 0.7270942337271296),\n",
       " ('criticisms', 0.7045277321940145),\n",
       " ('capitalist', 0.6621936235916519),\n",
       " ('fascism', 0.5649384550241625),\n",
       " ('anarchist', 0.5273477653209915),\n",
       " ('marxist', 0.5183360211065385)]"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.nearest_words(\"anarchism\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ukraine', 0.6713467751746154),\n",
       " ('russia', 0.6293371632068598),\n",
       " ('poland', 0.5504100164313704),\n",
       " ('belarus', 0.5384153768354997),\n",
       " ('yugoslavia', 0.538206195562453),\n",
       " ('romania', 0.517163403912315),\n",
       " ('serbia', 0.5066973712518864),\n",
       " ('austria', 0.49986558762309935),\n",
       " ('hungary', 0.46617419642332525),\n",
       " ('bulgaria', 0.42983248370363153)]"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.nearest_words(\"ussr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hop', 0.8283432918295232),\n",
       " ('hip', 0.8165784239590589),\n",
       " ('funk', 0.7577628668280326),\n",
       " ('rock', 0.7365094164302762),\n",
       " ('punk', 0.7063020806986656),\n",
       " ('music', 0.6759486602583847),\n",
       " ('pop', 0.6662904298474825),\n",
       " ('scene', 0.6589626455111179),\n",
       " ('band', 0.656824445005816),\n",
       " ('jazz', 0.6249604429360376)]"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.nearest_words(\"rap\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
